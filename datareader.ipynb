{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**工作记录 - CIFAR100InstanceSample函数调试与自定义数据集读取**\n",
    "\n",
    "**日期：** 2024年3月29日\n",
    "\n",
    "**目标：**\n",
    "\n",
    "调试CIFAR100InstanceSample函数，使其能够成功读取并处理自定义的数据集。\n",
    "\n",
    "**工作内容：**\n",
    "\n",
    "1. **理解函数原理**\n",
    "   - 查阅CIFAR100InstanceSample函数代码，理解其读取和处理CIFAR-100数据集的方式。\n",
    "\n",
    "2. **准备自定义数据集**\n",
    "   - 整理自定义数据集的文件夹结构，确保与CIFAR-100格式一致。\n",
    "   - 检查数据集中的图像文件，确保格式正确。\n",
    "\n",
    "3. **修改函数以适应自定义数据集**\n",
    "   - 修改数据集的路径为自定义数据集的存放位置。\n",
    "   - 调整数据加载逻辑，以匹配自定义数据集的格式。\n",
    "   - 更新数据预处理步骤，以符合自定义数据集的特点。\n",
    "\n",
    "4. **测试与调试**\n",
    "   - 运行修改后的CIFAR100InstanceSample函数，测试读取自定义数据集的功能。\n",
    "   - 根据测试结果调试代码，解决读取过程中遇到的问题。\n",
    "\n",
    "**备注：**\n",
    "\n",
    "- 调试过程中，主要关注数据集的路径、格式和加载逻辑。\n",
    "- 后续可进一步优化数据预处理步骤，提高数据处理效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import socket\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"\n",
    "mean = {\n",
    "    'cifar100': (0.5071, 0.4867, 0.4408),\n",
    "}\n",
    "\n",
    "std = {\n",
    "    'cifar100': (0.2675, 0.2565, 0.2761),\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_data_folder():\n",
    "    \"\"\"\n",
    "    return server-dependent path to store the data\n",
    "    \"\"\"\n",
    "    hostname = socket.gethostname()\n",
    "    if hostname.startswith('visiongpu'):\n",
    "        data_folder = '/data/vision/phillipi/rep-learn/datasets'\n",
    "    elif hostname.startswith('yonglong-home'):\n",
    "        data_folder = '/home/yonglong/Data/data'\n",
    "    else:\n",
    "        data_folder = './data/'\n",
    "\n",
    "    if not os.path.isdir(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "\n",
    "    return data_folder\n",
    "\n",
    "\n",
    "class CIFAR100Instance(datasets.CIFAR100):\n",
    "    \"\"\"CIFAR100Instance Dataset.\n",
    "    \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super().__getitem__(index)\n",
    "        return img, target, index\n",
    "\n",
    "\n",
    "def get_cifar100_dataloaders(batch_size=128, num_workers=8, is_instance=False):\n",
    "    \"\"\"\n",
    "    cifar 100\n",
    "    \"\"\"\n",
    "    data_folder = get_data_folder()\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "    ])\n",
    "\n",
    "    if is_instance:\n",
    "        train_set = CIFAR100Instance(root=data_folder,\n",
    "                                     download=True,\n",
    "                                     train=True,\n",
    "                                     transform=train_transform)\n",
    "        n_data = len(train_set)\n",
    "    else:\n",
    "        train_set = datasets.CIFAR100(root=data_folder,\n",
    "                                      download=True,\n",
    "                                      train=True,\n",
    "                                      transform=train_transform)\n",
    "    train_loader = DataLoader(train_set,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=num_workers)\n",
    "\n",
    "    test_set = datasets.CIFAR100(root=data_folder,\n",
    "                                 download=True,\n",
    "                                 train=False,\n",
    "                                 transform=test_transform)\n",
    "    test_loader = DataLoader(test_set,\n",
    "                             batch_size=int(batch_size/2),\n",
    "                             shuffle=False,\n",
    "                             num_workers=int(num_workers/2))\n",
    "\n",
    "    if is_instance:\n",
    "        return train_loader, test_loader, n_data\n",
    "    else:\n",
    "        return train_loader, test_loader\n",
    "\n",
    "\n",
    "class CIFAR100InstanceSample(datasets.CIFAR100):\n",
    "    def __init__(self, root, train=True,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False, k=4096, mode='exact', is_sample=True, percent=1.0):\n",
    "        super().__init__(root=root, train=train, download=download,\n",
    "                         transform=transform, target_transform=target_transform)\n",
    "        self.k = k\n",
    "        self.mode = mode\n",
    "        self.is_sample = is_sample\n",
    "\n",
    "        num_classes = 100\n",
    "        num_samples = len(self.data)\n",
    "        label = self.targets\n",
    "\n",
    "        self.cls_positive = [[] for i in range(num_classes)]\n",
    "        for i in range(num_samples):\n",
    "            self.cls_positive[label[i]].append(i)\n",
    "\n",
    "        self.cls_negative = [[] for i in range(num_classes)]\n",
    "        for i in range(num_classes):\n",
    "            for j in range(num_classes):\n",
    "                if j == i:\n",
    "                    continue\n",
    "                self.cls_negative[i].extend(self.cls_positive[j])\n",
    "\n",
    "        self.cls_positive = [np.asarray(self.cls_positive[i]) for i in range(num_classes)]\n",
    "        self.cls_negative = [np.asarray(self.cls_negative[i]) for i in range(num_classes)]\n",
    "\n",
    "        if 0 < percent < 1:\n",
    "            n = int(len(self.cls_negative[0]) * percent)\n",
    "            self.cls_negative = [np.random.permutation(self.cls_negative[i])[0:n] for i in range(num_classes)]\n",
    "\n",
    "        self.cls_positive = np.asarray(self.cls_positive)\n",
    "        self.cls_negative = np.asarray(self.cls_negative)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        img = Image.fromarray(img)  # 将图片从数组转换为PIL图像\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        if not self.is_sample:\n",
    "            return img, target, index  # 直接返回\n",
    "        else:\n",
    "            if self.mode == 'exact':\n",
    "                pos_idx = index\n",
    "            elif self.mode == 'relax':\n",
    "                pos_idx = np.random.choice(self.cls_positive[target], 1)\n",
    "                pos_idx = pos_idx[0]\n",
    "            else:\n",
    "                raise NotImplementedError(self.mode)\n",
    "\n",
    "            replace = True if self.k > len(self.cls_negative[target]) else False\n",
    "            neg_idx = np.random.choice(self.cls_negative[target], self.k, replace=replace)\n",
    "            sample_idx = np.hstack((np.asarray([pos_idx]), neg_idx))\n",
    "            return img, target, index, sample_idx\n",
    "\n",
    "\n",
    "def get_cifar100_dataloaders_sample(batch_size=128, num_workers=8, k=4096, mode='exact',\n",
    "                                    is_sample=True, percent=1.0):\n",
    "    data_folder = get_data_folder()\n",
    "    # 用于下面的transform参数，这里暂时不使用传统增强技术\n",
    "    train_transform = transforms.Compose([\n",
    "        # transforms.RandomCrop(32, padding=4),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "    ])\n",
    "    # 原：CIFAR100InstanceSample —> 需要修改成自己的版本\n",
    "    train_set = CIFAR100InstanceSample(root=data_folder,\n",
    "                                       download=True,\n",
    "                                       train=True,\n",
    "                                       transform=train_transform,\n",
    "                                       k=k,\n",
    "                                       mode=mode,\n",
    "                                       is_sample=is_sample,\n",
    "                                       percent=percent)\n",
    "    n_data = len(train_set)\n",
    "    # 查看n_data情况\n",
    "    # print(\"n_data\",n_data)\n",
    "    train_loader = DataLoader(train_set,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=num_workers)\n",
    "\n",
    "    test_set = datasets.CIFAR100(root=data_folder,\n",
    "                                 download=True,\n",
    "                                 train=False,\n",
    "                                 transform=test_transform)\n",
    "    test_loader = DataLoader(test_set,\n",
    "                             batch_size=int(batch_size/2),\n",
    "                             shuffle=False,\n",
    "                             num_workers=int(num_workers/2))\n",
    "\n",
    "    return train_loader, test_loader, n_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDatasetWithSampling(Dataset):\n",
    "    def __init__(self, root_dir, k=4096, mode='exact', is_sample=True, percent=1.0, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): 数据集目录路径。\n",
    "            k (int): 负样本的采样数。\n",
    "            mode (string): 正样本采样模式 ('exact'或'relax')。\n",
    "            is_sample (bool): 是否进行样本采样。\n",
    "            percent (float): 负样本采样百分比。\n",
    "            transform (callable, optional): 应用于样本的可选变换。\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.k = k\n",
    "        self.mode = mode\n",
    "        self.is_sample = is_sample\n",
    "        self.transform = transform\n",
    "\n",
    "        # 遍历数据集目录，收集所有图像的路径和相应的标签\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        for label in range(5):  # 假设有5个类别\n",
    "            label_folder = os.path.join(self.root_dir, str(label))\n",
    "            for img_file in os.listdir(label_folder):\n",
    "                if img_file.endswith('.png') or img_file.endswith('.jpg'):\n",
    "                    self.images.append(os.path.join(label_folder, img_file))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "        # 创建正样本和负样本的索引\n",
    "        self.cls_positive = [[] for _ in range(5)]\n",
    "        self.cls_negative = [[] for _ in range(5)]\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            self.cls_positive[label].append(idx)\n",
    "            for other_label in range(5):\n",
    "                if other_label != label:\n",
    "                    self.cls_negative[label].append(idx)\n",
    "\n",
    "        # 转换为numpy数组，以便于采样\n",
    "        self.cls_positive = [np.array(self.cls_positive[i]) for i in range(5)]\n",
    "        self.cls_negative = [np.array(self.cls_negative[i]) for i in range(5)]\n",
    "\n",
    "        # 调整负样本的数量\n",
    "        if 0 < percent < 1:\n",
    "            for i in range(5):\n",
    "                n = int(len(self.cls_negative[i]) * percent)\n",
    "                self.cls_negative[i] = np.random.permutation(self.cls_negative[i])[:n]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.images[index]\n",
    "        image = Image.open(img_path).convert('L')  # 转换为单通道图像\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if not self.is_sample:\n",
    "            return image, label, index\n",
    "\n",
    "        # 根据mode选择正样本索引\n",
    "        if self.mode == 'exact':\n",
    "            pos_idx = index\n",
    "        elif self.mode == 'relax':\n",
    "            pos_idx = np.random.choice(self.cls_positive[label], 1)[0]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Sampling mode not implemented.\")\n",
    "\n",
    "        # 选择负样本索引\n",
    "        replace = self.k > len(self.cls_negative[label])\n",
    "        neg_idx = np.random.choice(self.cls_negative[label], self.k, replace=replace)\n",
    "        sample_idx = np.hstack((np.array([pos_idx]), neg_idx))\n",
    "\n",
    "        return image, label, index, sample_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (initial_conv): ConvBlock(\n",
      "    (conv): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (hierarchical_block): HierarchicalSplitBlock(\n",
      "    (path1): ConvBlock(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (path2): ConvBlock(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (final_conv): ConvBlock(\n",
      "    (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (classifier): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, activation='relu'):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif self.activation == 'mish':\n",
    "            return x * torch.tanh(F.softplus(x))  # Mish activation function\n",
    "        elif self.activation == 'hard_swish':\n",
    "            return x * F.relu6(x + 3) / 6  # Hard Swish activation function\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class HierarchicalSplitBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(HierarchicalSplitBlock, self).__init__()\n",
    "        self.split_channels = channels // 2  # Assuming an equal split for simplicity\n",
    "        \n",
    "        # Doubling the paths or increasing the channel output\n",
    "        self.path1 = ConvBlock(self.split_channels, self.split_channels * 2, 3, padding=1, activation='mish')\n",
    "        self.path2 = ConvBlock(self.split_channels, self.split_channels * 2, 3, padding=1, activation='mish')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split feature maps\n",
    "        x1, x2 = x.chunk(2, dim=1)\n",
    "        \n",
    "        # Process through different paths\n",
    "        x1 = self.path1(x1)\n",
    "        x2 = self.path2(x2)\n",
    "        \n",
    "        # Concatenate results\n",
    "        out = torch.cat([x1, x2], dim=1)\n",
    "        return out\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Network, self).__init__()\n",
    "        self.initial_conv = ConvBlock(3, 64, 1)  # Adjust according to input channel size\n",
    "        self.hierarchical_block = HierarchicalSplitBlock(64)\n",
    "        self.final_conv = ConvBlock(128, 128, 1)  # Adjusted to maintain channel depth\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # Global Average Pooling\n",
    "        self.classifier = nn.Linear(128, num_classes)  # Classifier layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_conv(x)\n",
    "        x = self.hierarchical_block(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for the classifier\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Example use of the network\n",
    "num_classes = 10  # Assuming 10 classes for this example\n",
    "model = Network(num_classes)\n",
    "print(model)\n",
    "\n",
    "# Example input tensor of shape (1, 3, 32, 32)\n",
    "input_tensor = torch.rand(1, 3, 32, 32)\n",
    "\n",
    "# Forward pass through the network\n",
    "output = model(input_tensor)\n",
    "print(\"Output shape:\", output.shape)  # Should be [1, num_classes] indicating the class scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# 数据增强\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m transform \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     99\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(),\n\u001b[1;32m    100\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomCrop(\u001b[38;5;241m32\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m    101\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m    102\u001b[0m ])\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# 创建自定义数据集和数据加载器\u001b[39;00m\n\u001b[1;32m    105\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m)  \u001b[38;5;66;03m# 示例数据\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# SE注意力机制模块\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1)\n",
    "        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.mean(dim=(2, 3), keepdim=True)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = torch.sigmoid(self.fc2(out))\n",
    "        return x * out\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, activation='relu'):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif self.activation == 'mish':\n",
    "            return x * torch.tanh(F.softplus(x))  # Mish activation function\n",
    "        elif self.activation == 'hard_swish':\n",
    "            return x * F.relu6(x + 3) / 6  # Hard Swish activation function\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class HierarchicalSplitBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(HierarchicalSplitBlock, self).__init__()\n",
    "        self.split_channels = channels // 2\n",
    "        self.path1 = ConvBlock(self.split_channels, self.split_channels, 1, activation='relu')\n",
    "        self.path2 = ConvBlock(self.split_channels, self.split_channels, 1, activation='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2 = x.chunk(2, dim=1)\n",
    "        x1 = self.path1(x1)\n",
    "        x2 = self.path2(x2)\n",
    "        out = torch.cat([x1, x2], dim=1)\n",
    "        return out\n",
    "\n",
    "class BalancedLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(BalancedLayer, self).__init__()\n",
    "        self.fc = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = self.softmax(self.fc(x.mean(dim=(2, 3), keepdim=True)))\n",
    "        return x * weights\n",
    "\n",
    "class Hsnet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Hsnet, self).__init__()\n",
    "        self.initial_conv = ConvBlock(3, 32, 1)  # Reduced initial channels\n",
    "        self.hierarchical_block = HierarchicalSplitBlock(32)\n",
    "        self.se_block = SEBlock(32)\n",
    "        self.balanced_layer = BalancedLayer(32)\n",
    "        self.final_conv = ConvBlock(32, 64, 1)  # Adjusted final channels\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_conv(x)\n",
    "        x = self.hierarchical_block(x)\n",
    "        x = self.se_block(x)\n",
    "        x = self.balanced_layer(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# 示例训练循环\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# 数据增强\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 创建自定义数据集和数据加载器\n",
    "data = torch.randn(1000, 3, 32, 32)  # 示例数据\n",
    "labels = torch.randint(0, 10, (1000,))  # 示例标签\n",
    "dataset = CustomDataset(data, labels, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 计算权重并创建损失函数\n",
    "class_weights = calculate_class_weights(labels)\n",
    "criterion = WeightedCrossEntropyLoss(class_weights)\n",
    "\n",
    "# 创建模型和优化器\n",
    "model = Hsnet(num_classes=10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "train_model(model, train_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output: tensor([[-0.0382,  0.1532,  0.1824,  ...,  0.2673, -0.1966,  0.3453],\n",
      "        [-0.0425,  0.2310,  0.0764,  ...,  0.1513, -0.1323,  0.4988],\n",
      "        [-0.0453,  0.2080,  0.0991,  ...,  0.1463, -0.1402,  0.3995],\n",
      "        ...,\n",
      "        [-0.0988,  0.1952,  0.2048,  ...,  0.3187, -0.0922,  0.3970],\n",
      "        [-0.0826,  0.1793,  0.0706,  ...,  0.2799, -0.1421,  0.3789],\n",
      "        [-0.1023,  0.2133,  0.1069,  ...,  0.2155, -0.1124,  0.4104]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Soft Label: tensor([[0.0009, 0.0011, 0.0011,  ..., 0.0013, 0.0008, 0.0014],\n",
      "        [0.0009, 0.0012, 0.0010,  ..., 0.0011, 0.0008, 0.0016],\n",
      "        [0.0009, 0.0012, 0.0011,  ..., 0.0011, 0.0008, 0.0014],\n",
      "        ...,\n",
      "        [0.0009, 0.0012, 0.0012,  ..., 0.0013, 0.0009, 0.0014],\n",
      "        [0.0009, 0.0011, 0.0010,  ..., 0.0013, 0.0008, 0.0014],\n",
      "        [0.0009, 0.0012, 0.0011,  ..., 0.0012, 0.0009, 0.0014]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Soft No Softmax: tensor([[-0.0382,  0.1532,  0.1824,  ...,  0.2673, -0.1966,  0.3453],\n",
      "        [-0.0425,  0.2310,  0.0764,  ...,  0.1513, -0.1323,  0.4988],\n",
      "        [-0.0453,  0.2080,  0.0991,  ...,  0.1463, -0.1402,  0.3995],\n",
      "        ...,\n",
      "        [-0.0988,  0.1952,  0.2048,  ...,  0.3187, -0.0922,  0.3970],\n",
      "        [-0.0826,  0.1793,  0.0706,  ...,  0.2799, -0.1421,  0.3789],\n",
      "        [-0.1023,  0.2133,  0.1069,  ...,  0.2155, -0.1124,  0.4104]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def forward(self, x, before=False):\n",
    "        # 获取模型的最终输出\n",
    "        final_output = self.base_model(x)\n",
    "        \n",
    "        if before:\n",
    "            # 假设soft_label是经过softmax的概率分布\n",
    "            soft_label = F.softmax(final_output, dim=1)\n",
    "            \n",
    "            # 假设soft_no_softmax是未经处理的logits\n",
    "            soft_no_softmax = final_output\n",
    "            \n",
    "            return final_output, soft_label, soft_no_softmax\n",
    "        else:\n",
    "            return final_output\n",
    "\n",
    "# 创建一个自定义模型实例\n",
    "base_model = timm.create_model('resnet50', pretrained=False)\n",
    "model = CustomModel(base_model)\n",
    "\n",
    "# 输入图像的模拟张量\n",
    "images = torch.randn(8, 3, 224, 224)\n",
    "\n",
    "# 调用模型并获取输出\n",
    "outputs = model(images, before=True)\n",
    "output, soft_label, soft_no_softmax = outputs\n",
    "\n",
    "print(\"Final Output:\", output)\n",
    "print(\"Soft Label:\", soft_label)\n",
    "print(\"Soft No Softmax:\", soft_no_softmax)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
